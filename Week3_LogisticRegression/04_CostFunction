Cost Function:

1.  We know that the linear regression cost function is 1/2(h(x)-y)^2.
2.  We know that h(x) is g(theta^T x).
3.  If we minimize this cost function directly then it will give us a
    non-convex function with many local minima.
4.  This is because h(x) = 1/(1+e^-(theta^T x)) is non linear.
5.  If we run gradient descent on this non-convex function then its not going to
    converge to global minima.
6.  This is why we use a different cost function for logistic regression.
7.  Cost(h(x),y) = -log(h(x)) if y=1
                 = -(log(1-h(x))) if y=0
8.  Check "CostFunction.png" and CostFunction2.png" for plot.
9.  Keeping cost function like this guarantees that our cost function is convex.
10. When y=1 then if h(x)=1 then cost is minimized.
11. When y=1 then if h(x)=0 then cost is infinite.
12. When y=0 then if h(x)=0 then cost is minimized.
13. When y=0 then if h(x)=1 then cost is infinite.

Cost Function Simplified:

1.  The cost function can be minimized as:
    Cost(h(x)) = -y*log(h(x)) - (-y)log(1-h(x)) .
2.  
